{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model.BandaiDataset as bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"./saved_models\"\n",
    "DATASET_DIR = \"./datasets/data/\"\n",
    "FILELIST_PATH = \"datafiles.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = bd.BandaiDataset(FILELIST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "set_frame = 50\n",
    "def custom_collate_fn(batch):\n",
    "    motion_batch_tensor = torch.FloatTensor(len(batch),50,480,640)\n",
    "    motion_tensors = []\n",
    "    labels = []\n",
    "    #print(type(batch))\n",
    "\n",
    "    for item in batch:\n",
    "        motion_tensor = item.get_motion_tensor(50) # load an motion as a tensor(frames,width,height)\n",
    "        motion_tensors.append(motion_tensor.unsqueeze(0)) # put motions into a list : to be checked \n",
    "        labels.append(item.label)\n",
    "\n",
    "    torch.cat(motion_tensors, out=motion_batch_tensor)\n",
    "    label_batch_tensor = torch.LongTensor(labels)\n",
    "    return (motion_batch_tensor,label_batch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import multiprocessing as mp\n",
    "\n",
    "def load_data(file_list_path= '', data_path='', batch_sz = 5, train_val_test_split = [0.7,0.1,0.2]):\n",
    "    assert sum(train_val_test_split) == 1, \"Train, val and test fractions should sum to 1!\" \n",
    "    dataset = bd.BandaiDataset(data_path)\n",
    "    dataset.load()\n",
    "\n",
    "    tr_va_te = []\n",
    "    n_cpus = mp.cpu_count()\n",
    "    \n",
    "    for frac in train_val_test_split:\n",
    "        num = round(frac * dataset.num_of_files)\n",
    "        tr_va_te.append(num)\n",
    "    \n",
    "    if tr_va_te[0] != (dataset.num_of_files - tr_va_te[1] - tr_va_te[2]):\n",
    "        tr_va_te[0] = (dataset.num_of_files - tr_va_te[1] - tr_va_te[2])\n",
    "    \n",
    "    train_split, val_split, test_split = random_split(dataset, tr_va_te)\n",
    "\n",
    "    train_dl = DataLoader(train_split,\n",
    "                          batch_size=batch_sz,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=custom_collate_fn,\n",
    "                          num_workers=n_cpus\n",
    "                        )\n",
    "    val_dl = DataLoader(val_split,\n",
    "                        batch_size=batch_sz,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=custom_collate_fn,\n",
    "                        num_workers=n_cpus)\n",
    "    test_dl = DataLoader(test_split,\n",
    "                         batch_size=batch_sz,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=custom_collate_fn,\n",
    "                         num_workers=n_cpus)\n",
    "\n",
    "    return train_dl, val_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_motions, _ = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_motions,test_labels = next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for (motion_batch,label_batch) in train_dl:\n",
    "    batch_sz = len(motion_batch)\n",
    "    print(batch_sz)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.size(dataset[164].pose_list[0]))\n",
    "print(dataset[164].frame_num)\n",
    "#tensor = np.array(dataset[54].pose_list)\n",
    "for pose in dataset[54].pose_list:\n",
    "    print(np.size(pose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset.filelist))\n",
    "print(dataset.filelist[164])\n",
    "dataset.num_of_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,165):\n",
    "    print(f'{i}: {dataset[i].frame_num} ->' ,end='')\n",
    "    tensor = dataset[i].get_motion_tensor(50)\n",
    "    print({dataset[i].frame_num},flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.classification as tmcls \n",
    "class ClassifierMetrics(object):\n",
    "    ap: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1: float\n",
    "    acc: float\n",
    "    count: int\n",
    "\n",
    "    def __init__(self, task, n_labels, device):\n",
    "        self.task = task\n",
    "        if self.task == \"multiclass\":\n",
    "            self.ap_metric = tmcls.MulticlassAveragePrecision(num_classes=n_labels, average=None, thresholds=None).to(device)\n",
    "            self.precision_metric = tmcls.MulticlassPrecision(num_classes=n_labels).to(device)\n",
    "            self.recall_metric = tmcls.MulticlassRecall(num_classes=n_labels).to(device)\n",
    "            self.f1_metric = tmcls.MulticlassF1Score(num_classes=n_labels).to(device)\n",
    "            self.acc_metric = tmcls.MulticlassAccuracy(num_classes=n_labels).to(device)\n",
    "\n",
    "        elif self.task == \"multilabel\":\n",
    "            self.ap_metric = tmcls.MultilabelAveragePrecision(num_labels=n_labels, average=None, thresholds=None).to(device)\n",
    "            self.precision_metric = tmcls.MultilabelPrecision(num_labels=n_labels).to(device)\n",
    "            self.recall_metric = tmcls.MultilabelRecall(num_labels=n_labels).to(device)\n",
    "            self.f1_metric = tmcls.MultilabelF1Score(num_labels=n_labels).to(device)\n",
    "            self.acc_metric = tmcls.MultilabelAccuracy(task=self.task, num_labels=n_labels).to(device)\n",
    "        self.reset()\n",
    "    \n",
    "\n",
    "    def reset(self):\n",
    "        self.ap = 0\n",
    "        self.precision = 0\n",
    "        self.recall = 0\n",
    "        self.f1 = 0\n",
    "        self.acc = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, y_pred, y):\n",
    "        y = y.long()\n",
    "        self.ap += self.ap_metric(y_pred, y)\n",
    "        self.precision += self.precision_metric(y_pred, y)\n",
    "        self.recall += self.recall_metric(y_pred, y)\n",
    "        self.f1 += self.f1_metric(y_pred, y)\n",
    "        self.acc += self.acc_metric(y_pred, y)\n",
    "        self.count += 1 #y.size(0)\n",
    "\n",
    "    def calc(self, y_pred, y):\n",
    "        self.reset()\n",
    "        y = y.long()\n",
    "        self.ap = self.ap_metric(y_pred, y)\n",
    "        self.precision = self.precision_metric(y_pred, y)\n",
    "        self.recall = self.recall_metric(y_pred, y)\n",
    "        self.f1 = self.f1_metric(y_pred, y)\n",
    "\n",
    "    def avg(self):\n",
    "        self.ap = self.ap / self.count\n",
    "        self.precision = self.precision / self.count\n",
    "        self.recall = self.recall / self.count\n",
    "        self.f1 = self.f1 / self.count\n",
    "        self.acc = self.acc / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=50, out_channels=50, kernel_size=5, stride=1, padding=2) #[(dimension_sz−kernel+2*Padding)/Stride]+1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=50, out_channels=50, kernel_size=5, stride=1, padding=2) #[(dimension_sz−kernel+2*Padding)/Stride]+1\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)  \n",
    "##########\n",
    "        '''\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=5, stride=1, padding=2) #[(dimension_sz−kernel+2*Padding)/Stride]+1\n",
    "        self.relu3 = nn.ReLU()\n",
    "        '''\n",
    "        #self.pool3 = nn.MaxPool2d(kernel_size=2)  \n",
    "        \n",
    "        self.flat = nn.Flatten()            \n",
    "        self.fc1 = nn.Linear(50 * 160 * 120, 128)   \n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = self.conv1(inp)\n",
    "        inp = self.relu1(inp) \n",
    "        inp = self.pool1(inp)\n",
    "\n",
    "        inp = self.conv2(inp)\n",
    "        inp = self.relu2(inp) \n",
    "        inp = self.pool2(inp)\n",
    "        '''\n",
    "        inp = self.conv3(inp)\n",
    "        inp = self.relu3(inp) \n",
    "        '''\n",
    "        #inp = self.pool3(inp)\n",
    "        \n",
    "\n",
    "        inp = self.flat(inp)\n",
    "\n",
    "        inp = self.fc1 (inp) \n",
    "        inp = self.relu4(inp)            \n",
    "        inp = self.fc2(inp) \n",
    "        inp = self.relu5(inp)\n",
    "        inp = self.fc3(inp) \n",
    "        inp = self.relu6(inp)\n",
    "        out = self.fc4(inp)\n",
    "        return out\n",
    "    \n",
    "\n",
    "def get_simple_conv_net():\n",
    "    return ConvNet()\n",
    "\n",
    "summary(get_simple_conv_net(), input_size=(50, 640, 480), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs,train_dl,optimiser, device):\n",
    "    msg = \"\"\n",
    "    for epoch in range (epochs):\n",
    "        total_steps = len(train_dl)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "        for batch_num, (motion_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(motion_batch)\n",
    "            motion_batch  = motion_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            output = model(motion_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "            optimiser.zero_grad()\n",
    "            losses.backward\n",
    "            optimiser.step()\n",
    "\n",
    "            preds = torch.argmax(output,dim=1)\n",
    "            correct += int(torch.eq(preds,label_batch).sum())\n",
    "            total += batch_sz\n",
    "            minibatch_accuracy = 100 * correct/total\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "epochs = 50\n",
    "batch_sz = 16\n",
    "learning_rate = 0.00005\n",
    "train_dl, val_dl, test_dl = load_data(batch_sz=batch_sz)\n",
    "network = get_simple_conv_net().to(DEVICE)\n",
    "optim = SGD(network.parameters(),lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(network,epochs,train_dl,optim, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model  = get_simple_conv_net()\n",
    "optimizer = SGD(model.parameters(), lr = learning_rate)\n",
    "gamma = 0.5\n",
    "schd = ExponentialLR(optimizer, gamma)\n",
    "\n",
    "lrs = []\n",
    "for i in range (epochs):\n",
    "    optimizer.step()\n",
    "    lrs.append(schd.get_last_lr())\n",
    "    schd.step()\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train' : train_dl,\n",
    "    'val' : val_dl,\n",
    "    'test' :test_dl\n",
    "}\n",
    "\n",
    "def train_model_gpu_lr_conv_valid(model, epochs, dataloaders, optimiser, lr_scheduler):\n",
    "    msg = ''\n",
    "    for epoch in range(epochs):\n",
    "        ################ TRAINING ################\n",
    "        model.train()\n",
    "        train_dl = dataloaders[\"train\"]\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch_num, (motion_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(motion_batch)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            motion_batch = motion_batch.to(DEVICE)\n",
    "            output = model(motion_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            preds_train = torch.argmax(output, dim=1)\n",
    "            correct_train += int(torch.eq(preds_train, label_batch).sum())\n",
    "            total_train +=batch_sz\n",
    "            minibatch_accuracy_train = 100 * correct_train / total_train\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_train}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy_train:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        ##############################################################\n",
    "        print(\"\") # Create newline between progress bars\n",
    "        ##############################################################\n",
    "\n",
    "        model.eval()\n",
    "        val_dl = dataloaders[\"val\"]\n",
    "        total_val = len(val_dl)\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        for batch_num, (motion_batch, label_batch) in enumerate(val_dl):\n",
    "            batch_sz = len(motion_batch)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            motion_batch = motion_batch.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                output = model(motion_batch)\n",
    "                losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                preds_val = torch.argmax(output, dim=1)\n",
    "                correct_val += int(torch.eq(preds_val, label_batch).sum())\n",
    "                total_val += batch_sz\n",
    "                minibatch_accuracy_val = 100 * correct_val / total_val\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            #if (batch_num + 1) % 5 == 0:\n",
    "            print(\" \" * len(msg), end='\\r')\n",
    "            msg = f'Val epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_val}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy_val:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "            print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "     \n",
    "        ########################################################################\n",
    "        print(\"\")  # Create newline between progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate simple conv net\n",
    "network = get_simple_conv_net()\n",
    "# instantiate SGD optimiser\n",
    "optim = SGD(network.parameters(),lr = learning_rate)\n",
    "# instantiate exponential learning rate scheduler\n",
    "lr_sch = ExponentialLR(optim,gamma)\n",
    "# move model to DEVICE\n",
    "network = network.to(DEVICE)\n",
    "# call latest training function\n",
    "train_model_gpu_lr_conv_valid(network,epochs,dataloaders,optim,lr_sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience =  1, tolerance = 0):\n",
    "        self.patience = patience\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "        self.epoch_counter = 0\n",
    "        self.max_validation_acc = np.NINF\n",
    "\n",
    "    def should_stop(self, validation_acc):\n",
    "        if validation_acc > self.max_validation_acc:\n",
    "            self.max_validation_acc = validation_acc\n",
    "            self.epoch_counter = 0\n",
    "        elif validation_acc < (self.max_validation_acc - self.tolerance):\n",
    "            self.epoch_counter += 1\n",
    "            if(self.epoch_counter >= self.patience):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CHECKPOINT_DIR = './checkpoints'\n",
    "def save_checkpoint(model, epoch, save_dir):\n",
    "    filename = f\"checkout_{epoch}.pth\"\n",
    "    save_path = f\"{save_dir}/{filename}\"\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_final(model, epochs, dataloders,\n",
    "                      optimizer, lr_scheduler, writer,\n",
    "                      early_stopper,checkpoint_frequency):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):\n",
    "        ################# TRAINING ####################\n",
    "        model.train()\n",
    "        train_dl = dataloaders['train']\n",
    "\n",
    "        total_steps_train = len(train_dl)\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        loss_train = 0\n",
    "\n",
    "        for batch_num, (motion_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(motion_batch)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            motion_batch = motion_batch.to(DEVICE)\n",
    "            output = model(motion_batch)\n",
    "            loss_train = nn.CrossEntropyLoss()(output, label_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds_train = torch.argmax(output, dim=1)\n",
    "            correct_train += int(torch.eq(preds_train, label_batch).sum())\n",
    "            total_train += batch_sz\n",
    "            minibatch_accuracy_train = 100 * correct_train / total_train\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_train}], Loss: {loss_train.item():.5f}, Acc: {minibatch_accuracy_train:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "        lr_scheduler.step()\n",
    "        ########################################################################\n",
    "        print(\"\") # Create newline between progress bars\n",
    "        #######################VALIDATION STEP##################################\n",
    "        model.eval()\n",
    "        val_dl = dataloaders['val']\n",
    "\n",
    "        total_steps_val = len(val_dl)\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        loss_val = 0\n",
    "\n",
    "        for batch_num, (motion_batch,label_batch) in enumerate(val_dl):\n",
    "            batch_sz = len(motion_batch)\n",
    "            motion_batch = motion_batch.to(DEVICE)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(motion_batch)\n",
    "                loss_val = nn.CrossEntropyLoss()(output, label_batch)\n",
    "\n",
    "                preds_val = torch.argmax(output, dim = 1)\n",
    "\n",
    "                correct_val += int(torch.eq(preds_val, label_batch).sum())\n",
    "                total_val += batch_sz\n",
    "                minibatch_accuracy_train = 100 * correct_val / total_val\n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "                if (batch_num + 1) % 2 == 0:\n",
    "                    print(\" \" * len(msg), end='\\r')\n",
    "                    msg = f'Eval epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_val}], Loss: {loss_val.item():.5f}, Acc: {minibatch_accuracy_val:.5f}'\n",
    "                    if early_stopper.epoch_counter > 0:\n",
    "                        msg += f\", Epochs without improvement: {early_stopper.epoch_counter}\"\n",
    "                    print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "        ########################################################################\n",
    "        print(\"\")  # Create newline between progress bars\n",
    "\n",
    "        epoch_train_acc = 100 * correct_train / total_steps_train\n",
    "        epoch_val_acc = 100 * correct_val/ total_steps_val\n",
    "        writer.add_scalar(\"loss/train\",loss_train,epoch)\n",
    "        writer.add_scalar(\"loss/train\",loss_val,epoch)\n",
    "        writer.add_scalar(\"Acc/train\",epoch_train_acc,epoch)\n",
    "        writer.add_scalar(\"Acc/val\", epoch_val_acc,epoch)\n",
    "\n",
    "        if epoch % checkpoint_frequency == 0:\n",
    "            save_checkpoint(model, epoch, \"./saved_models\")\n",
    "        if early_stopper.should_stop(epoch_val_acc):\n",
    "            print(f\"\\nValidation accuracy has not improved in {early_stopper.epoch_counter} epochs, stopping.\")\n",
    "            save_checkpoint(model,epoch,\"./saved_models\")\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epochs = 60\n",
    "batch_sz = 10\n",
    "checkpoint_frequency = 3\n",
    "\n",
    "train_dl, val_dl, test_dl = load_data()\n",
    "\n",
    "dataloaders = {\n",
    "    'train' : train_dl,\n",
    "    'val' : val_dl,\n",
    "    'test' : test_dl\n",
    "}\n",
    "\n",
    "network = get_simple_conv_net()\n",
    "network = network.to(DEVICE)\n",
    "\n",
    "optim = SGD(network.parameters(), lr=learning_rate)\n",
    "lr_sch = ExponentialLR(optim, gamma)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "stopp = EarlyStopper(patience = 5, tolerance = 2)\n",
    "#train_model_final(network,epochs,dataloaders,optim,lr_sch,writer,stopp,checkpoint_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model.mymodel as mymodel\n",
    "last_epoch = 57\n",
    "loaded_net_state_dic = torch.load(f\"./saved_models/checkout_{last_epoch}.pth\")\n",
    "train_dl, val_dl, test_dl = mymodel.load_data()\n",
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    test_dl = dataloaders['test']\n",
    "    total_steps = len(test_dl)\n",
    "    msg = \"\"\n",
    "    for batch_num, (image_batch, label_batch) in enumerate(test_dl):\n",
    "        batch_sz = len(image_batch)\n",
    "        label_batch = label_batch.to(DEVICE)\n",
    "        image_batch = image_batch.to(DEVICE)\n",
    "        out = model(image_batch)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        correct += int(torch.eq(preds, label_batch).sum())\n",
    "        total += label_batch.shape[0]\n",
    "        if (batch_num + 1) % 5 == 0:\n",
    "            print(\" \" * len(msg), end='\\r')\n",
    "            msg = f'Testing batch[{batch_num + 1}/{total_steps}]'\n",
    "            print (msg, end='\\r' if batch_num < total_steps else \"\\n\", flush=True)\n",
    "    print(f\"\\nFinal test accuracy for {total} examples: {100 * correct/total:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_simple_conv_net()\n",
    "model = model.to(DEVICE)\n",
    "model.load_state_dict(loaded_net_state_dic)\n",
    "test_model(model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-envrionment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
