{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model.BandaiDataset as bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"./saved_models\"\n",
    "DATASET_DIR = \"./datasets/data/\"\n",
    "FILELIST_PATH = \"datafiles.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = bd.BandaiDataset(FILELIST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "set_frame = 50\n",
    "def custom_collate_fn(batch):\n",
    "    motion_batch_tensor = torch.FloatTensor(len(batch),50,480,640)\n",
    "    motion_tensors = []\n",
    "    labels = []\n",
    "    #print(type(batch))\n",
    "\n",
    "    for item in batch:\n",
    "        #print(item)\n",
    "        motion_tensor = item.get_motion_tensor(50) # load an motion as a tensor(frames,width,height)\n",
    "        motion_tensors.append(motion_tensor.unsqueeze(0)) # put motions into a list : to be checked \n",
    "        labels.append(item.label)\n",
    "\n",
    "    torch.cat(motion_tensors, out=motion_batch_tensor)\n",
    "    label_batch_tensor = torch.LongTensor(labels)\n",
    "    return (motion_batch_tensor,label_batch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import multiprocessing as mp\n",
    "\n",
    "def load_data(file_list_path= '', data_path='', batch_sz = 5, train_val_test_split = [0.7,0.1,0.2]):\n",
    "    assert sum(train_val_test_split) == 1, \"Train, val and test fractions should sum to 1!\" \n",
    "    dataset = bd.BandaiDataset(data_path)\n",
    "    dataset.load()\n",
    "\n",
    "    tr_va_te = []\n",
    "    n_cpus = mp.cpu_count()\n",
    "    \n",
    "    for frac in train_val_test_split:\n",
    "        num = round(frac * dataset.num_of_files)\n",
    "        tr_va_te.append(num)\n",
    "    \n",
    "    if tr_va_te[0] != (dataset.num_of_files - tr_va_te[1] - tr_va_te[2]):\n",
    "        tr_va_te[0] = (dataset.num_of_files - tr_va_te[1] - tr_va_te[2])\n",
    "    \n",
    "    #assert 1==2, f\"tr_va_te = {tr_va_te}\"\n",
    "\n",
    "    train_split, val_split, test_split = random_split(dataset, tr_va_te)\n",
    "\n",
    "    train_dl = DataLoader(train_split,\n",
    "                          batch_size=batch_sz,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=custom_collate_fn,\n",
    "                          num_workers=n_cpus\n",
    "                        )\n",
    "    val_dl = DataLoader(val_split,\n",
    "                        batch_size=batch_sz,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=custom_collate_fn,\n",
    "                        num_workers=n_cpus)\n",
    "    test_dl = DataLoader(test_split,\n",
    "                         batch_size=batch_sz,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=custom_collate_fn,\n",
    "                         num_workers=n_cpus)\n",
    "\n",
    "    return train_dl, val_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_motions, _ = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_motions,test_labels = next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor (motion_batch,label_batch) in train_dl:\\n    batch_sz = len(motion_batch)\\n    print(batch_sz)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for (motion_batch,label_batch) in train_dl:\n",
    "    batch_sz = len(motion_batch)\n",
    "    print(batch_sz)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307200\n",
      "227\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n",
      "307200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.size(dataset[164].pose_list[0]))\n",
    "print(dataset[164].frame_num)\n",
    "#tensor = np.array(dataset[54].pose_list)\n",
    "for pose in dataset[54].pose_list:\n",
    "    print(np.size(pose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "dataset-1_walk_tired_002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset.filelist))\n",
    "print(dataset.filelist[164])\n",
    "dataset.num_of_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 129 ->{50}\n",
      "1: 91 ->{50}\n",
      "2: 99 ->{50}\n",
      "3: 118 ->{50}\n",
      "4: 219 ->{50}\n",
      "5: 123 ->{50}\n",
      "6: 139 ->{50}\n",
      "7: 71 ->{50}\n",
      "8: 157 ->{50}\n",
      "9: 139 ->{50}\n",
      "10: 146 ->{50}\n",
      "11: 173 ->{50}\n",
      "12: 138 ->{50}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13: 222 ->{50}\n",
      "14: 166 ->{50}\n",
      "15: 230 ->{50}\n",
      "16: 56 ->{50}\n",
      "17: 170 ->{50}\n",
      "18: 150 ->{50}\n",
      "19: 85 ->{50}\n",
      "20: 236 ->{50}\n",
      "21: 221 ->{50}\n",
      "22: 202 ->{50}\n",
      "23: 158 ->{50}\n",
      "24: 122 ->{50}\n",
      "25: 141 ->{50}\n",
      "26: 123 ->{50}\n",
      "27: 80 ->{50}\n",
      "28: 235 ->{50}\n",
      "29: 304 ->{50}\n",
      "30: 186 ->{50}\n",
      "31: 111 ->{50}\n",
      "32: 124 ->{50}\n",
      "33: 131 ->{50}\n",
      "34: 329 ->{50}\n",
      "35: 143 ->{50}\n",
      "36: 224 ->{50}\n",
      "37: 213 ->{50}\n",
      "38: 188 ->{50}\n",
      "39: 112 ->{50}\n",
      "40: 209 ->{50}\n",
      "41: 142 ->{50}\n",
      "42: 174 ->{50}\n",
      "43: 254 ->{50}\n",
      "44: 211 ->{50}\n",
      "45: 86 ->{50}\n",
      "46: 75 ->{50}\n",
      "47: 76 ->{50}\n",
      "48: 70 ->{50}\n",
      "49: 98 ->{50}\n",
      "50: 68 ->{50}\n",
      "51: 118 ->{50}\n",
      "52: 71 ->{50}\n",
      "53: 68 ->{50}\n",
      "54: 21 ->{50}\n",
      "55: 100 ->{50}\n",
      "56: 86 ->{50}\n",
      "57: 106 ->{50}\n",
      "58: 82 ->{50}\n",
      "59: 94 ->{50}\n",
      "60: 141 ->{50}\n",
      "61: 111 ->{50}\n",
      "62: 120 ->{50}\n",
      "63: 187 ->{50}\n",
      "64: 301 ->{50}\n",
      "65: 164 ->{50}\n",
      "66: 106 ->{50}\n",
      "67: 153 ->{50}\n",
      "68: 201 ->{50}\n",
      "69: 87 ->{50}\n",
      "70: 163 ->{50}\n",
      "71: 216 ->{50}\n",
      "72: 114 ->{50}\n",
      "73: 164 ->{50}\n",
      "74: 83 ->{50}\n",
      "75: 73 ->{50}\n",
      "76: 78 ->{50}\n",
      "77: 112 ->{50}\n",
      "78: 71 ->{50}\n",
      "79: 121 ->{50}\n",
      "80: 80 ->{50}\n",
      "81: 141 ->{50}\n",
      "82: 60 ->{50}\n",
      "83: 73 ->{50}\n",
      "84: 25 ->{50}\n",
      "85: 142 ->{50}\n",
      "86: 103 ->{50}\n",
      "87: 142 ->{50}\n",
      "88: 142 ->{50}\n",
      "89: 134 ->{50}\n",
      "90: 208 ->{50}\n",
      "91: 257 ->{50}\n",
      "92: 241 ->{50}\n",
      "93: 187 ->{50}\n",
      "94: 316 ->{50}\n",
      "95: 237 ->{50}\n",
      "96: 216 ->{50}\n",
      "97: 184 ->{50}\n",
      "98: 201 ->{50}\n",
      "99: 32 ->{50}\n",
      "100: 365 ->{50}\n",
      "101: 283 ->{50}\n",
      "102: 392 ->{50}\n",
      "103: 393 ->{50}\n",
      "104: 218 ->{50}\n",
      "105: 91 ->{50}\n",
      "106: 152 ->{50}\n",
      "107: 132 ->{50}\n",
      "108: 119 ->{50}\n",
      "109: 218 ->{50}\n",
      "110: 118 ->{50}\n",
      "111: 150 ->{50}\n",
      "112: 105 ->{50}\n",
      "113: 148 ->{50}\n",
      "114: 136 ->{50}\n",
      "115: 243 ->{50}\n",
      "116: 157 ->{50}\n",
      "117: 204 ->{50}\n",
      "118: 335 ->{50}\n",
      "119: 139 ->{50}\n",
      "120: 91 ->{50}\n",
      "121: 153 ->{50}\n",
      "122: 138 ->{50}\n",
      "123: 119 ->{50}\n",
      "124: 222 ->{50}\n",
      "125: 156 ->{50}\n",
      "126: 153 ->{50}\n",
      "127: 103 ->{50}\n",
      "128: 149 ->{50}\n",
      "129: 136 ->{50}\n",
      "130: 243 ->{50}\n",
      "131: 137 ->{50}\n",
      "132: 265 ->{50}\n",
      "133: 290 ->{50}\n",
      "134: 135 ->{50}\n",
      "135: 147 ->{50}\n",
      "136: 146 ->{50}\n",
      "137: 144 ->{50}\n",
      "138: 243 ->{50}\n",
      "139: 236 ->{50}\n",
      "140: 227 ->{50}\n",
      "141: 171 ->{50}\n",
      "142: 185 ->{50}\n",
      "143: 278 ->{50}\n",
      "144: 211 ->{50}\n",
      "145: 195 ->{50}\n",
      "146: 223 ->{50}\n",
      "147: 214 ->{50}\n",
      "148: 136 ->{50}\n",
      "149: 166 ->{50}\n",
      "150: 187 ->{50}\n",
      "151: 269 ->{50}\n",
      "152: 192 ->{50}\n",
      "153: 196 ->{50}\n",
      "154: 32 ->{50}\n",
      "155: 482 ->{50}\n",
      "156: 172 ->{50}\n",
      "157: 286 ->{50}\n",
      "158: 296 ->{50}\n",
      "159: 332 ->{50}\n",
      "160: 305 ->{50}\n",
      "161: 348 ->{50}\n",
      "162: 412 ->{50}\n",
      "163: 248 ->{50}\n",
      "164: 227 ->{50}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,165):\n",
    "    print(f'{i}: {dataset[i].frame_num} ->' ,end='')\n",
    "    tensor = dataset[i].get_motion_tensor(50)\n",
    "    print({dataset[i].frame_num},flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.classification as tmcls \n",
    "class ClassifierMetrics(object):\n",
    "    ap: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1: float\n",
    "    acc: float\n",
    "    count: int\n",
    "\n",
    "    def __init__(self, task, n_labels, device):\n",
    "        self.task = task\n",
    "        if self.task == \"multiclass\":\n",
    "            self.ap_metric = tmcls.MulticlassAveragePrecision(num_classes=n_labels, average=None, thresholds=None).to(device)\n",
    "            self.precision_metric = tmcls.MulticlassPrecision(num_classes=n_labels).to(device)\n",
    "            self.recall_metric = tmcls.MulticlassRecall(num_classes=n_labels).to(device)\n",
    "            self.f1_metric = tmcls.MulticlassF1Score(num_classes=n_labels).to(device)\n",
    "            self.acc_metric = tmcls.MulticlassAccuracy(num_classes=n_labels).to(device)\n",
    "\n",
    "        elif self.task == \"multilabel\":\n",
    "            self.ap_metric = tmcls.MultilabelAveragePrecision(num_labels=n_labels, average=None, thresholds=None).to(device)\n",
    "            self.precision_metric = tmcls.MultilabelPrecision(num_labels=n_labels).to(device)\n",
    "            self.recall_metric = tmcls.MultilabelRecall(num_labels=n_labels).to(device)\n",
    "            self.f1_metric = tmcls.MultilabelF1Score(num_labels=n_labels).to(device)\n",
    "            self.acc_metric = tmcls.MultilabelAccuracy(task=self.task, num_labels=n_labels).to(device)\n",
    "        self.reset()\n",
    "    \n",
    "\n",
    "    def reset(self):\n",
    "        self.ap = 0\n",
    "        self.precision = 0\n",
    "        self.recall = 0\n",
    "        self.f1 = 0\n",
    "        self.acc = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, y_pred, y):\n",
    "        y = y.long()\n",
    "        self.ap += self.ap_metric(y_pred, y)\n",
    "        self.precision += self.precision_metric(y_pred, y)\n",
    "        self.recall += self.recall_metric(y_pred, y)\n",
    "        self.f1 += self.f1_metric(y_pred, y)\n",
    "        self.acc += self.acc_metric(y_pred, y)\n",
    "        self.count += 1 #y.size(0)\n",
    "\n",
    "    def calc(self, y_pred, y):\n",
    "        self.reset()\n",
    "        y = y.long()\n",
    "        self.ap = self.ap_metric(y_pred, y)\n",
    "        self.precision = self.precision_metric(y_pred, y)\n",
    "        self.recall = self.recall_metric(y_pred, y)\n",
    "        self.f1 = self.f1_metric(y_pred, y)\n",
    "\n",
    "    def avg(self):\n",
    "        self.ap = self.ap / self.count\n",
    "        self.precision = self.precision / self.count\n",
    "        self.recall = self.recall / self.count\n",
    "        self.f1 = self.f1 / self.count\n",
    "        self.acc = self.acc / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 50, 640, 480]          62,550\n",
      "              ReLU-2         [-1, 50, 640, 480]               0\n",
      "         MaxPool2d-3         [-1, 50, 320, 240]               0\n",
      "            Conv2d-4         [-1, 50, 320, 240]          62,550\n",
      "              ReLU-5         [-1, 50, 320, 240]               0\n",
      "         MaxPool2d-6         [-1, 50, 160, 120]               0\n",
      "           Flatten-7               [-1, 960000]               0\n",
      "            Linear-8                  [-1, 128]     122,880,128\n",
      "              ReLU-9                  [-1, 128]               0\n",
      "           Linear-10                  [-1, 256]          33,024\n",
      "             ReLU-11                  [-1, 256]               0\n",
      "           Linear-12                   [-1, 64]          16,448\n",
      "             ReLU-13                   [-1, 64]               0\n",
      "           Linear-14                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 123,055,350\n",
      "Trainable params: 123,055,350\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 58.59\n",
      "Forward/backward pass size (MB): 336.92\n",
      "Params size (MB): 469.42\n",
      "Estimated Total Size (MB): 864.93\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=50, out_channels=50, kernel_size=5, stride=1, padding=2) #[(dimension_sz−kernel+2*Padding)/Stride]+1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=50, out_channels=50, kernel_size=5, stride=1, padding=2) #[(dimension_sz−kernel+2*Padding)/Stride]+1\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)  \n",
    "##########\n",
    "        '''\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=5, stride=1, padding=2) #[(dimension_sz−kernel+2*Padding)/Stride]+1\n",
    "        self.relu3 = nn.ReLU()\n",
    "        '''\n",
    "        #self.pool3 = nn.MaxPool2d(kernel_size=2)  \n",
    "        \n",
    "        self.flat = nn.Flatten()            \n",
    "        self.fc1 = nn.Linear(50 * 160 * 120, 128)   \n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = self.conv1(inp)\n",
    "        inp = self.relu1(inp) \n",
    "        inp = self.pool1(inp)\n",
    "\n",
    "        inp = self.conv2(inp)\n",
    "        inp = self.relu2(inp) \n",
    "        inp = self.pool2(inp)\n",
    "        '''\n",
    "        inp = self.conv3(inp)\n",
    "        inp = self.relu3(inp) \n",
    "        '''\n",
    "        #inp = self.pool3(inp)\n",
    "        \n",
    "\n",
    "        inp = self.flat(inp)\n",
    "\n",
    "        inp = self.fc1 (inp) \n",
    "        inp = self.relu4(inp)            \n",
    "        inp = self.fc2(inp) \n",
    "        inp = self.relu5(inp)\n",
    "        inp = self.fc3(inp) \n",
    "        inp = self.relu6(inp)\n",
    "        out = self.fc4(inp)\n",
    "        return out\n",
    "    \n",
    "\n",
    "def get_simple_conv_net():\n",
    "    return ConvNet()\n",
    "\n",
    "summary(get_simple_conv_net(), input_size=(50, 640, 480), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs,train_dl,optimiser, device):\n",
    "    msg = \"\"\n",
    "    for epoch in range (epochs):\n",
    "        total_steps = len(train_dl)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "        for batch_num, (motion_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(motion_batch)\n",
    "            motion_batch  = motion_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            output = model(motion_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "            optimiser.zero_grad()\n",
    "            losses.backward\n",
    "            optimiser.step()\n",
    "\n",
    "            preds = torch.argmax(output,dim=1)\n",
    "            correct += int(torch.eq(preds,label_batch).sum())\n",
    "            total += batch_sz\n",
    "            minibatch_accuracy = 100 * correct/total\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "epochs = 50\n",
    "batch_sz = 16\n",
    "learning_rate = 0.00005\n",
    "train_dl, val_dl, test_dl = load_data(batch_sz=batch_sz)\n",
    "network = get_simple_conv_net().to(DEVICE)\n",
    "optim = SGD(network.parameters(),lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(network,epochs,train_dl,optim, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff163f50a50>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model  = get_simple_conv_net()\n",
    "optimizer = SGD(model.parameters(), lr = learning_rate)\n",
    "gamma = 0.5\n",
    "schd = ExponentialLR(optimizer, gamma)\n",
    "\n",
    "lrs = []\n",
    "for i in range (epochs):\n",
    "    optimizer.step()\n",
    "    lrs.append(schd.get_last_lr())\n",
    "    schd.step()\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train' : train_dl,\n",
    "    'val' : val_dl,\n",
    "    'test' :test_dl\n",
    "}\n",
    "\n",
    "def train_model_gpu_lr_conv_valid(model, epochs, dataloaders, optimiser, lr_scheduler):\n",
    "    msg = ''\n",
    "    for epoch in range(epochs):\n",
    "        ################ TRAINING ################\n",
    "        model.train()\n",
    "        train_dl = dataloaders[\"train\"]\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch_num, (motion_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(motion_batch)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            motion_batch = motion_batch.to(DEVICE)\n",
    "            output = model(motion_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            preds_train = torch.argmax(output, dim=1)\n",
    "            correct_train += int(torch.eq(preds_train, label_batch).sum())\n",
    "            total_train +=batch_sz\n",
    "            minibatch_accuracy_train = 100 * correct_train / total_train\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_train}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy_train:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        ##############################################################\n",
    "        print(\"\") # Create newline between progress bars\n",
    "        ##############################################################\n",
    "\n",
    "        model.eval()\n",
    "        val_dl = dataloaders[\"val\"]\n",
    "        total_val = len(val_dl)\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        for batch_num, (motion_batch, label_batch) in enumerate(val_dl):\n",
    "            batch_sz = len(motion_batch)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            motion_batch = motion_batch.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                output = model(motion_batch)\n",
    "                losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                preds_val = torch.argmax(output, dim=1)\n",
    "                correct_val += int(torch.eq(preds_val, label_batch).sum())\n",
    "                total_val += batch_sz\n",
    "                minibatch_accuracy_val = 100 * correct_val / total_val\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            #if (batch_num + 1) % 5 == 0:\n",
    "            print(\" \" * len(msg), end='\\r')\n",
    "            msg = f'Val epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_val}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy_val:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "            print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "     \n",
    "        ########################################################################\n",
    "        print(\"\")  # Create newline between progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch[1/50], MiniBatch[5/80], Loss: 3.24829, Acc: 13.75000, LR: 0.00005\n",
      "Val epoch[1/50], MiniBatch[1/16], Loss: 3.56038, Acc: 12.50000, LR: 0.00003  \n",
      "Train epoch[2/50], MiniBatch[5/80], Loss: 2.44013, Acc: 12.50000, LR: 0.00003\n",
      "Val epoch[2/50], MiniBatch[1/16], Loss: 2.47062, Acc: 12.50000, LR: 0.00001  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m network \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m      9\u001b[0m \u001b[39m# call latest training function\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m train_model_gpu_lr_conv_valid(network,epochs,dataloaders,optim,lr_sch)\n",
      "Cell \u001b[0;32mIn[21], line 28\u001b[0m, in \u001b[0;36mtrain_model_gpu_lr_conv_valid\u001b[0;34m(model, epochs, dataloaders, optimiser, lr_scheduler)\u001b[0m\n\u001b[1;32m     25\u001b[0m optimiser\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m preds_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m correct_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(torch\u001b[39m.\u001b[39;49meq(preds_train, label_batch)\u001b[39m.\u001b[39;49msum())\n\u001b[1;32m     29\u001b[0m total_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39mbatch_sz\n\u001b[1;32m     30\u001b[0m minibatch_accuracy_train \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m \u001b[39m*\u001b[39m correct_train \u001b[39m/\u001b[39m total_train\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# instantiate simple conv net\n",
    "network = get_simple_conv_net()\n",
    "# instantiate SGD optimiser\n",
    "optim = SGD(network.parameters(),lr = learning_rate)\n",
    "# instantiate exponential learning rate scheduler\n",
    "lr_sch = ExponentialLR(optim,gamma)\n",
    "# move model to DEVICE\n",
    "network = network.to(DEVICE)\n",
    "# call latest training function\n",
    "train_model_gpu_lr_conv_valid(network,epochs,dataloaders,optim,lr_sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience =  1, tolerance = 0):\n",
    "        self.patience = patience\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "        self.epoch_counter = 0\n",
    "        self.max_validation_acc = np.NINF\n",
    "\n",
    "    def should_stop(self, validation_acc):\n",
    "        if validation_acc > self.max_validation_acc:\n",
    "            self.max_validation_acc = validation_acc\n",
    "            self.epoch_counter = 0\n",
    "        elif validation_acc < (self.max_validation_acc - self.tolerance):\n",
    "            self.epoch_counter += 1\n",
    "            if(self.epoch_counter >= self.patience):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CHECKPOINT_DIR = './checkpoints'\n",
    "def save_checkpoint(model, epoch, save_dir):\n",
    "    filename = f\"checkout_{epoch}.pth\"\n",
    "    save_path = f\"{save_dir}/{filename}\"\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_final(model, epochs, dataloders,\n",
    "                      optimizer, lr_scheduler, writer,\n",
    "                      early_stopper,checkpoint_frequency):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):\n",
    "        ################# TRAINING ####################\n",
    "        model.train()\n",
    "        train_dl = dataloaders['train']\n",
    "\n",
    "        total_steps_train = len(train_dl)\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        loss_train = 0\n",
    "\n",
    "        for batch_num, (motion_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(motion_batch)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            motion_batch = motion_batch.to(DEVICE)\n",
    "            output = model(motion_batch)\n",
    "            loss_train = nn.CrossEntropyLoss()(output, label_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds_train = torch.argmax(output, dim=1)\n",
    "            correct_train += int(torch.eq(preds_train, label_batch).sum())\n",
    "            total_train += batch_sz\n",
    "            minibatch_accuracy_train = 100 * correct_train / total_train\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_train}], Loss: {loss_train.item():.5f}, Acc: {minibatch_accuracy_train:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "        lr_scheduler.step()\n",
    "        ########################################################################\n",
    "        print(\"\") # Create newline between progress bars\n",
    "        #######################VALIDATION STEP##################################\n",
    "        model.eval()\n",
    "        val_dl = dataloaders['val']\n",
    "\n",
    "        total_steps_val = len(val_dl)\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        loss_val = 0\n",
    "\n",
    "        for batch_num, (motion_batch,label_batch) in enumerate(val_dl):\n",
    "            batch_sz = len(motion_batch)\n",
    "            motion_batch = motion_batch.to(DEVICE)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(motion_batch)\n",
    "                loss_val = nn.CrossEntropyLoss()(output, label_batch)\n",
    "\n",
    "                preds_val = torch.argmax(output, dim = 1)\n",
    "\n",
    "                correct_val += int(torch.eq(preds_val, label_batch).sum())\n",
    "                total_val += batch_sz\n",
    "                minibatch_accuracy_train = 100 * correct_val / total_val\n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "                if (batch_num + 1) % 2 == 0:\n",
    "                    print(\" \" * len(msg), end='\\r')\n",
    "                    msg = f'Eval epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_val}], Loss: {loss_val.item():.5f}, Acc: {minibatch_accuracy_val:.5f}'\n",
    "                    if early_stopper.epoch_counter > 0:\n",
    "                        msg += f\", Epochs without improvement: {early_stopper.epoch_counter}\"\n",
    "                    print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "        ########################################################################\n",
    "        print(\"\")  # Create newline between progress bars\n",
    "\n",
    "        epoch_train_acc = 100 * correct_train / total_steps_train\n",
    "        epoch_val_acc = 100 * correct_val/ total_steps_val\n",
    "        writer.add_scalar(\"loss/train\",loss_train,epoch)\n",
    "        writer.add_scalar(\"loss/train\",loss_val,epoch)\n",
    "        writer.add_scalar(\"Acc/train\",epoch_train_acc,epoch)\n",
    "        writer.add_scalar(\"Acc/val\", epoch_val_acc,epoch)\n",
    "\n",
    "        if epoch % checkpoint_frequency == 0:\n",
    "            save_checkpoint(model, epoch, \"./saved_models\")\n",
    "        if early_stopper.should_stop(epoch_val_acc):\n",
    "            print(f\"\\nValidation accuracy has not improved in {early_stopper.epoch_counter} epochs, stopping.\")\n",
    "            save_checkpoint(model,epoch,\"./saved_models\")\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch[1/60], MiniBatch[20/24], Loss: 3.29868, Acc: 13.00000, LR: 0.00005\n",
      "\n",
      "Train epoch[2/60], MiniBatch[20/24], Loss: 2.28613, Acc: 16.00000, LR: 0.00003\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter()\n\u001b[1;32m     22\u001b[0m stopp \u001b[39m=\u001b[39m EarlyStopper(patience \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, tolerance \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m train_model_final(network,epochs,dataloaders,optim,lr_sch,writer,stopp,checkpoint_frequency)\n",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m, in \u001b[0;36mtrain_model_final\u001b[0;34m(model, epochs, dataloders, optimizer, lr_scheduler, writer, early_stopper, checkpoint_frequency)\u001b[0m\n\u001b[1;32m     13\u001b[0m loss_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m batch_num, (motion_batch, label_batch) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dl):\n\u001b[0;32m---> 16\u001b[0m     batch_sz \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(motion_batch)\n\u001b[1;32m     17\u001b[0m     label_batch \u001b[39m=\u001b[39m label_batch\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     18\u001b[0m     motion_batch \u001b[39m=\u001b[39m motion_batch\u001b[39m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment-envrionment/lib/python3.11/site-packages/torch/_tensor.py:964\u001b[0m, in \u001b[0;36mTensor.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[39m__neg__\u001b[39m \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_TensorBase\u001b[39m.\u001b[39mneg\n\u001b[1;32m    962\u001b[0m \u001b[39m__abs__\u001b[39m \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_TensorBase\u001b[39m.\u001b[39mabs\n\u001b[0;32m--> 964\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    965\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    966\u001b[0m         \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "epochs = 60\n",
    "batch_sz = 10\n",
    "checkpoint_frequency = 3\n",
    "\n",
    "train_dl, val_dl, test_dl = load_data()\n",
    "\n",
    "dataloaders = {\n",
    "    'train' : train_dl,\n",
    "    'val' : val_dl,\n",
    "    'test' : test_dl\n",
    "}\n",
    "\n",
    "network = get_simple_conv_net()\n",
    "network = network.to(DEVICE)\n",
    "\n",
    "optim = SGD(network.parameters(), lr=learning_rate)\n",
    "lr_sch = ExponentialLR(optim, gamma)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "stopp = EarlyStopper(patience = 5, tolerance = 2)\n",
    "#train_model_final(network,epochs,dataloaders,optim,lr_sch,writer,stopp,checkpoint_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model.mymodel as mymodel\n",
    "last_epoch = 57\n",
    "loaded_net_state_dic = torch.load(f\"./saved_models/checkout_{last_epoch}.pth\")\n",
    "train_dl, val_dl, test_dl = mymodel.load_data()\n",
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    test_dl = dataloaders['test']\n",
    "    total_steps = len(test_dl)\n",
    "    msg = \"\"\n",
    "    for batch_num, (image_batch, label_batch) in enumerate(test_dl):\n",
    "        batch_sz = len(image_batch)\n",
    "        label_batch = label_batch.to(DEVICE)\n",
    "        image_batch = image_batch.to(DEVICE)\n",
    "        out = model(image_batch)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        correct += int(torch.eq(preds, label_batch).sum())\n",
    "        total += label_batch.shape[0]\n",
    "        if (batch_num + 1) % 5 == 0:\n",
    "            print(\" \" * len(msg), end='\\r')\n",
    "            msg = f'Testing batch[{batch_num + 1}/{total_steps}]'\n",
    "            print (msg, end='\\r' if batch_num < total_steps else \"\\n\", flush=True)\n",
    "    print(f\"\\nFinal test accuracy for {total} examples: {100 * correct/total:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch[5/7]\n",
      "Final test accuracy for 33 examples: 27.27273\n"
     ]
    }
   ],
   "source": [
    "model = get_simple_conv_net()\n",
    "model = model.to(DEVICE)\n",
    "model.load_state_dict(loaded_net_state_dic)\n",
    "test_model(model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-envrionment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
